{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5qc03aytXHk"
      },
      "source": [
        "#utliser les robots pour scrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract(url):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=15)\n",
        "    except requests.RequestException:\n",
        "        return None\n",
        "\n",
        "    ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
        "    if \"text/html\" not in ctype:\n",
        "        return None  # on ignore tout ce qui n'est pas du HTML\n",
        "\n",
        "    # r.text peut bug selon encodage; on force un décodage safe\n",
        "    try:\n",
        "        html = r.content.decode(r.encoding or \"utf-8\", errors=\"ignore\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "    for tag in soup([\"header\", \"footer\", \"nav\", \"script\", \"style\"]):\n",
        "        tag.decompose()\n",
        "\n",
        "    title = (soup.title.get_text(strip=True) if soup.title else \"\").strip()\n",
        "    text = soup.get_text(\"\\n\", strip=True)\n",
        "\n",
        "    if len(text) < 200:\n",
        "        return None\n",
        "\n",
        "    return {\"url\": url, \"title\": title, \"text\": text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_urls(main_sitemap):\n",
        "    try:\n",
        "        r = requests.get(main_sitemap, timeout=15)\n",
        "    except requests.RequestException:\n",
        "        return []\n",
        "\n",
        "    # sitemap = XML (text)\n",
        "    try:\n",
        "        xml = r.content.decode(r.encoding or \"utf-8\", errors=\"ignore\")\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(xml, \"xml\")\n",
        "    sitemap_links = [loc.get_text(strip=True) for loc in soup.find_all(\"loc\")]\n",
        "\n",
        "    all_urls = []\n",
        "    for sitemap in sitemap_links:\n",
        "        try:\n",
        "            r2 = requests.get(sitemap, timeout=15)\n",
        "        except requests.RequestException:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            xml2 = r2.content.decode(r2.encoding or \"utf-8\", errors=\"ignore\")\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        sub = BeautifulSoup(xml2, \"xml\")\n",
        "        all_urls.extend([loc.get_text(strip=True) for loc in sub.find_all(\"loc\")])\n",
        "\n",
        "    return all_urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "esilv: 7664 URLs -> fichier: esilv_all_pages.jsonl\n",
            "emlv: 0 URLs -> fichier: emlv_all_pages.jsonl\n",
            "iim: 0 URLs -> fichier: iim_all_pages.jsonl\n",
            "VF global: all_sites_VF.jsonl\n"
          ]
        }
      ],
      "source": [
        "SITEMAPS = {\n",
        "    \"esilv\": \"https://www.esilv.fr/sitemap_index.xml\",\n",
        "    #\"emlv\": \"https://www.emlv.fr/sitemap_index.xml\",\n",
        "    #\"iim\": \"https://www.iim.fr/sitemap_index.xml\",\n",
        "}\n",
        "\n",
        "VF_PATH = \"all_sites_VF.jsonl\"\n",
        "\n",
        "# reset du VF à chaque run\n",
        "open(VF_PATH, \"w\", encoding=\"utf-8\").close()\n",
        "\n",
        "for name, sitemap in SITEMAPS.items():\n",
        "    urls = get_all_urls(sitemap)\n",
        "\n",
        "    with open(f\"{name}_all_pages.jsonl\", \"w\", encoding=\"utf-8\") as f_site, \\\n",
        "         open(VF_PATH, \"a\", encoding=\"utf-8\") as f_vf:\n",
        "\n",
        "        for url in urls:\n",
        "            doc = extract(url)\n",
        "            if doc:\n",
        "                line = json.dumps(doc, ensure_ascii=False) + \"\\n\"\n",
        "                f_site.write(line)\n",
        "                f_vf.write(line)\n",
        "\n",
        "    print(f\"{name}: {len(urls)} URLs -> fichier: {name}_all_pages.jsonl\")\n",
        "\n",
        "print(f\"VF global: {VF_PATH}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py312",
      "language": "python",
      "name": "py312"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
