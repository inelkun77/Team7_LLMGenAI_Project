{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json, requests\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "metadata": {
        "id": "sGU9fEgyWOQe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URLS = [\n",
        "    \"https://www.esilv.fr/\",\n",
        "    \"https://www.esilv.fr/lecole/\",\n",
        "    \"https://www.esilv.fr/admissions/\",\n",
        "    \"https://www.esilv.fr/formations/cycle-ingenieur/\",\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "-9SkTaEYWRDy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract(url):\n",
        "    try:\n",
        "        html = requests.get(url, timeout=15).text  # télécharge le contenu HTML de la page\n",
        "    except:\n",
        "        return None\n",
        "    soup = BeautifulSoup(html, \"html.parser\")      # parse le HTML pour le rendre lisible\n",
        "    for tag in soup([\"header\", \"footer\", \"nav\", \"script\", \"style\"]):\n",
        "        tag.decompose()                            # supprime les éléments inutiles (menus, scripts…)\n",
        "    title = (soup.title.get_text(strip=True) if soup.title else \"\").strip()  # récupère le titre\n",
        "    text = soup.get_text(\"\\n\", strip=True)         # extrait tout le texte visible\n",
        "    if len(text) < 200:                            # ignore les pages trop courtes\n",
        "        return None\n",
        "    return {\"url\": url, \"title\": title, \"text\": text}  # retourne les infos sous forme de dictionnaire\n",
        "\n",
        "with open(\"esilv_pages.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for url in URLS:\n",
        "        doc = extract(url)\n",
        "        if doc:\n",
        "            f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "qhEIm3FTWBa0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fonction d’extraction d’une page\n",
        "def extract(url):\n",
        "    try:\n",
        "        html = requests.get(url, timeout=15).text  # télécharge la page\n",
        "    except:\n",
        "        return None\n",
        "    soup = BeautifulSoup(html, \"html.parser\")      # parse le HTML\n",
        "    for tag in soup([\"header\", \"footer\", \"nav\", \"script\", \"style\"]):\n",
        "        tag.decompose()                            # supprime les parties inutiles\n",
        "    title = (soup.title.get_text(strip=True) if soup.title else \"\").strip()\n",
        "    text = soup.get_text(\"\\n\", strip=True)\n",
        "    if len(text) < 200:\n",
        "        return None\n",
        "    return {\"url\": url, \"title\": title, \"text\": text}"
      ],
      "metadata": {
        "id": "XJNm7c6DiA-U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#utliser les robots pour scrapper"
      ],
      "metadata": {
        "id": "T5qc03aytXHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# récupère toutes les URLs depuis le sitemap principal\n",
        "def get_all_urls(main_sitemap):\n",
        "    html = requests.get(main_sitemap, timeout=15).text\n",
        "    soup = BeautifulSoup(html, \"xml\")\n",
        "    sitemap_links = [loc.get_text() for loc in soup.find_all(\"loc\")]\n",
        "    all_urls = []\n",
        "    for sitemap in sitemap_links:\n",
        "        xml = requests.get(sitemap, timeout=15).text\n",
        "        sub = BeautifulSoup(xml, \"xml\")\n",
        "        all_urls += [loc.get_text() for loc in sub.find_all(\"loc\")]\n",
        "    return all_urls\n",
        "\n",
        "# lit toutes les URLs du site via le sitemap principal\n",
        "URLS = get_all_urls(\"https://www.esilv.fr/sitemap_index.xml\")\n",
        "\n",
        "with open(\"esilv_all_pages.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for url in URLS:\n",
        "        doc = extract(url)\n",
        "        if doc:\n",
        "            f.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "z7W84PjSYm6k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}